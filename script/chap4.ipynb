{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d72cc89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4-2 KNN\n",
    "import polars as pl\n",
    "from sklearn.neighbors import NearestNeighbors  # k-NN\n",
    "\n",
    "k_in_knn = 5  # k-NN における k\n",
    "\n",
    "dataset = pl.read_csv(\"../test_data/resin.csv\")\n",
    "x_prediction = pl.read_csv(\"../test_data/resin_prediction.csv\")\n",
    "\n",
    "# データ分割\n",
    "y = dataset.get_column(\"property\")  # 目的変数\n",
    "x = dataset.drop(y.name)  # 説明変数\n",
    "\n",
    "# 標準偏差が 0 の特徴量の削除\n",
    "deleting_variables = [\n",
    "    col for col, std in x.std().row(0, named=True).items() if std in [0, None]\n",
    "]\n",
    "x = x.drop(deleting_variables)\n",
    "x_prediction = x_prediction.drop(deleting_variables)\n",
    "\n",
    "# オートスケーリング\n",
    "autoscaled_x = x.select((pl.all() - pl.all().mean()) / pl.all().std())\n",
    "autoscaled_x_prediction = (\n",
    "    x_prediction - x.mean().select(pl.all().repeat_by(x_prediction.height).explode())\n",
    ") / x.std().select(pl.all().repeat_by(x_prediction.height).explode())\n",
    "\n",
    "# k-NN による AD\n",
    "ad_model = NearestNeighbors(n_neighbors=k_in_knn, metric=\"euclidean\")  # AD モデルの宣言\n",
    "ad_model.fit(\n",
    "    autoscaled_x\n",
    ")  # k-NN による AD では、トレーニングデータの x を model_ad に格納することに対応\n",
    "\n",
    "# サンプルごとの k 最近傍サンプルとの距離に加えて、k 最近傍サンプルのインデックス番号も一緒に出力されるため、出力用の変数を 2 つに\n",
    "# トレーニングデータでは k 最近傍サンプルの中に自分も含まれ、自分との距離の 0 を除いた距離を考える必要があるため、k_in_knn + 1 個と設定\n",
    "knn_distance_train, knn_index_train = ad_model.kneighbors(\n",
    "    autoscaled_x, n_neighbors=k_in_knn + 1\n",
    ")\n",
    "test = pl.DataFrame(knn_distance_train).drop(pl.first())\n",
    "mean_of_knn_distance_train = (\n",
    "    # 自分以外の k_in_knn 個の距離の平均\n",
    "    pl.DataFrame(knn_distance_train)\n",
    "    .drop(pl.first())\n",
    "    .mean_horizontal()\n",
    "    .alias(\"mean_of_knn_distance\")\n",
    ")\n",
    "mean_of_knn_distance_train.to_frame().write_csv(\n",
    "    \"../output/mean_of_knn_distance_train.csv\"\n",
    ")\n",
    "\n",
    "# AD 内となるトレーニングデータの割合。ADのしきい値を決めるときに使用\n",
    "rate_of_training_samples_inside_ad = 0.96\n",
    "# トレーニングデータのサンプルの rate_of_training_samples_inside_ad * 100 % が含まれるようにしきい値を設定\n",
    "ad_threshold: float = mean_of_knn_distance_train.sort().item(\n",
    "    # サンプル数の96%の位置の値を閾値とする\n",
    "    round(autoscaled_x.height * rate_of_training_samples_inside_ad) - 1\n",
    ")\n",
    "\n",
    "# トレーニングデータに対して、AD の中か外かを判定\n",
    "inside_ad_flag_train = mean_of_knn_distance_train.le(ad_threshold).alias(\n",
    "    \"inside_ad_flag\"\n",
    ")\n",
    "inside_ad_flag_train.to_frame().write_csv(\"../output/inside_ad_flag_train_knn.csv\")\n",
    "\n",
    "# 予測用データに対する k-NN 距離の計算\n",
    "knn_distance_prediction, knn_index_prediction = ad_model.kneighbors(\n",
    "    autoscaled_x_prediction\n",
    ")\n",
    "# k_in_knn 個の距離の平均\n",
    "mean_of_knn_distance_prediction = (\n",
    "    pl.DataFrame(knn_distance_prediction)\n",
    "    .mean_horizontal()\n",
    "    .alias(\"mean_of_knn_distance\")\n",
    ")\n",
    "mean_of_knn_distance_prediction.to_frame().write_csv(\n",
    "    \"../output/mean_of_knn_distance_prediction.csv\"\n",
    ")\n",
    "\n",
    "# 予測用データに対して、AD の中か外かを判定\n",
    "inside_ad_flag_prediction = mean_of_knn_distance_prediction.le(ad_threshold)\n",
    "inside_ad_flag_prediction.to_frame().write_csv(\n",
    "    \"../output/inside_ad_flag_prediction_knn.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e24f93be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "トレーニングデータにおけるサポートベクター数 :10\n",
      "トレーニングデータにおけるサポートベクターの割合 :0.5\n",
      "トレーニングデータにおける外れサンプル数 :4\n",
      "トレーニングデータにおける外れサンプルの割合 :0.2\n",
      "\n",
      "予測用データセットにおける外れサンプル数 :4757 \n",
      "予測用データセットにおける外れサンプルの割合 :0.5826800587947085\n"
     ]
    }
   ],
   "source": [
    "# 4-2 One-Class Support Vector Machine\n",
    "\n",
    "from sklearn.svm import OneClassSVM\n",
    "\n",
    "ocsvm_nu = 0.04  # OCSVM における ν。トレーニングデータにおけるサンプル数に対する、サポートベクターの数の下限の割合\n",
    "ocsvm_gamma = 0.1  # OCSVM における γ\n",
    "\n",
    "dataset = pl.read_csv(\"../test_data/resin.csv\")\n",
    "x_prediction = pl.read_csv(\"../test_data/resin_prediction.csv\")\n",
    "\n",
    "# データ分割\n",
    "y = dataset.get_column(\"property\")  # 目的変数\n",
    "x = dataset.drop(y.name)  # 説明変数\n",
    "\n",
    "# 標準偏差が 0 の特徴量の削除\n",
    "deleting_variables = [\n",
    "    col for col, std in x.std().row(0, named=True).items() if std in [0, None]\n",
    "]\n",
    "x = x.drop(deleting_variables)\n",
    "x_prediction = x_prediction.drop(deleting_variables)\n",
    "\n",
    "# オートスケーリング\n",
    "autoscaled_x = x.select((pl.all() - pl.all().mean()) / pl.all().std())\n",
    "autoscaled_x_prediction = (\n",
    "    x_prediction - x.mean().select(pl.all().repeat_by(x_prediction.height).explode())\n",
    ") / x.std().select(pl.all().repeat_by(x_prediction.height).explode())\n",
    "\n",
    "# OCSVM による AD\n",
    "ad_model = OneClassSVM(kernel=\"rbf\", gamma=ocsvm_gamma, nu=ocsvm_nu)  # AD モデルの宣言\n",
    "ad_model.fit(autoscaled_x)  # モデル構築\n",
    "\n",
    "# トレーニングデータのデータ密度 (f(x) の値)\n",
    "data_density_train = pl.Series(ad_model.decision_function(autoscaled_x)).alias(\n",
    "    \"ocsvm_data_density\"\n",
    ")\n",
    "data_density_train.to_frame().write_csv(\"../output/ocsvm_data_density_train.csv\")\n",
    "number_of_support_vectors = len(ad_model.support_)\n",
    "number_of_outliers_in_training_data = data_density_train.lt(0).sum()\n",
    "print(\n",
    "    f\"\\nトレーニングデータにおけるサポートベクター数 :{number_of_support_vectors}\"\n",
    "    f\"\\nトレーニングデータにおけるサポートベクターの割合 :{number_of_support_vectors / x.height}\"\n",
    "    f\"\\nトレーニングデータにおける外れサンプル数 :{number_of_outliers_in_training_data}\"\n",
    "    f\"\\nトレーニングデータにおける外れサンプルの割合 :{number_of_outliers_in_training_data / x.height}\"\n",
    ")\n",
    "\n",
    "# トレーニングデータに対して、AD の中か外かを判定\n",
    "data_density_train.ge(0).alias(\"inside_ad_flag\").to_frame().write_csv(\n",
    "    \"../output/inside_ad_flag_train_ocsvm.csv\"\n",
    ")\n",
    "\n",
    "# 予測用データセットのデータ密度 (f(x) の値)\n",
    "data_density_prediction = pl.Series(\n",
    "    ad_model.decision_function(autoscaled_x_prediction)\n",
    ").alias(\"ocsvm_data_density\")\n",
    "data_density_prediction.to_frame().write_csv(\n",
    "    \"../output/ocsvm_data_density_prediction.csv\"\n",
    ")\n",
    "number_of_outliers_in_prediction_data = data_density_prediction.lt(0).sum()\n",
    "print(\n",
    "    f\"\\n予測用データセットにおける外れサンプル数 :{number_of_outliers_in_prediction_data}\",\n",
    "    f\"\\n予測用データセットにおける外れサンプルの割合 :{number_of_outliers_in_prediction_data / x_prediction.height}\",\n",
    ")\n",
    "\n",
    "# 予測用データセットに対して、AD の中か外かを判定\n",
    "data_density_prediction.ge(0).alias(\"inside_ad_flag\").to_frame().write_csv(\n",
    "    \"../output/inside_ad_flag_prediction_ocsvm.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e4dd4dee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "最適化された gamma : 0.25\n",
      "\n",
      "トレーニングデータにおけるサポートベクター数 :14\n",
      "トレーニングデータにおけるサポートベクターの割合 :0.7\n",
      "トレーニングデータにおける外れサンプル数 :6\n",
      "トレーニングデータにおける外れサンプルの割合 :0.3\n",
      "\n",
      "予測用データセットにおける外れサンプル数 :6252 \n",
      "予測用データセットにおける外れサンプルの割合 :0.7658010779029887\n"
     ]
    }
   ],
   "source": [
    "# 4-2 One-Class Support Vector Machine Gamma optimization\n",
    "\n",
    "import polars as pl\n",
    "from sklearn.svm import OneClassSVM\n",
    "\n",
    "dataset = pl.read_csv(\"../test_data/resin.csv\")\n",
    "x_prediction = pl.read_csv(\"../test_data/resin_prediction.csv\")\n",
    "\n",
    "# データ分割\n",
    "y = dataset.get_column(\"property\")  # 目的変数\n",
    "x = dataset.drop(y.name)  # 説明変数\n",
    "\n",
    "# 標準偏差が 0 の特徴量の削除\n",
    "deleting_variables = [\n",
    "    col for col, std in x.std().row(0, named=True).items() if std in [0, None]\n",
    "]\n",
    "x = x.drop(deleting_variables)\n",
    "x_prediction = x_prediction.drop(deleting_variables)\n",
    "\n",
    "# オートスケーリング\n",
    "autoscaled_x = x.select((pl.all() - pl.all().mean()) / pl.all().std())\n",
    "autoscaled_x_prediction = (\n",
    "    x_prediction - x.mean().select(pl.all().repeat_by(x_prediction.height).explode())\n",
    ") / x.std().select(pl.all().repeat_by(x_prediction.height).explode())\n",
    "\n",
    "# 分散最大化によるガウシアンカーネルのγの最適化\n",
    "ocsvm_gammas = 2.0 ** pl.arange(-20, 11, eager=True)  # OCSVM における γ の候補\n",
    "autoscaled_x_per_sample = autoscaled_x.select(pl.concat_list(pl.all()).alias(\"sample\"))\n",
    "sample_distances = (\n",
    "    # すべてのサンプルを組み合わせる\n",
    "    autoscaled_x_per_sample.join(autoscaled_x_per_sample, how=\"cross\", suffix=\"_right\")\n",
    "    .select(\n",
    "        # 全サンプル間の全データのユークリッド距離の二乗を計算\n",
    "        (pl.col(\"sample\") - pl.col(\"sample_right\"))\n",
    "        .list.eval(pl.element().pow(2))\n",
    "        .list.sum()\n",
    "        .implode()  # グラム行列の計算を簡単にするために、全距離をリスト化\n",
    "        .alias(\"distance\"),\n",
    "    )\n",
    "    .to_series(0)\n",
    ")\n",
    "\n",
    "gram_matrix = (-ocsvm_gammas * sample_distances).list.eval(pl.element().exp())\n",
    "optimal_gamma: float = ocsvm_gammas.item(gram_matrix.list.var().arg_max())\n",
    "print(\"最適化された gamma :\", optimal_gamma)\n",
    "\n",
    "# OCSVM による AD\n",
    "ocsvm_nu = 0.04  # OCSVM における ν。トレーニングデータにおけるサンプル数に対する、サポートベクターの数の下限の割合\n",
    "# AD モデルの宣言\n",
    "ad_model = OneClassSVM(kernel=\"rbf\", gamma=optimal_gamma, nu=ocsvm_nu)\n",
    "ad_model.fit(autoscaled_x)  # モデル構築\n",
    "\n",
    "# トレーニングデータのデータ密度 (f(x) の値)\n",
    "data_density_train = pl.Series(ad_model.decision_function(autoscaled_x)).alias(\n",
    "    \"ocsvm_data_density\"\n",
    ")\n",
    "data_density_train.to_frame().write_csv(\n",
    "    \"../output/ocsvm_gamma_optimization_data_density_train.csv\"\n",
    ")\n",
    "number_of_support_vectors = len(ad_model.support_)\n",
    "number_of_outliers_in_training_data = data_density_train.lt(0).sum()\n",
    "print(\n",
    "    f\"\\nトレーニングデータにおけるサポートベクター数 :{number_of_support_vectors}\"\n",
    "    f\"\\nトレーニングデータにおけるサポートベクターの割合 :{number_of_support_vectors / x.height}\"\n",
    "    f\"\\nトレーニングデータにおける外れサンプル数 :{number_of_outliers_in_training_data}\"\n",
    "    f\"\\nトレーニングデータにおける外れサンプルの割合 :{number_of_outliers_in_training_data / x.height}\"\n",
    ")\n",
    "\n",
    "# トレーニングデータに対して、AD の中か外かを判定\n",
    "data_density_train.ge(0).alias(\"inside_ad_flag\").to_frame().write_csv(\n",
    "    \"../output/inside_ad_flag_train_ocsvm_gamma_optimization.csv\"\n",
    ")\n",
    "\n",
    "# 予測用データセットのデータ密度 (f(x) の値)\n",
    "data_density_prediction = pl.Series(\n",
    "    ad_model.decision_function(autoscaled_x_prediction)\n",
    ").alias(\"ocsvm_data_density\")\n",
    "data_density_prediction.to_frame().write_csv(\n",
    "    \"../output/ocsvm_gamma_optimization_data_density_prediction.csv\"\n",
    ")\n",
    "number_of_outliers_in_prediction_data = data_density_prediction.lt(0).sum()\n",
    "print(\n",
    "    f\"\\n予測用データセットにおける外れサンプル数 :{number_of_outliers_in_prediction_data}\",\n",
    "    f\"\\n予測用データセットにおける外れサンプルの割合 :{number_of_outliers_in_prediction_data / x_prediction.height}\",\n",
    ")\n",
    "\n",
    "# 予測用データセットに対して、AD の中か外かを判定\n",
    "data_density_prediction.ge(0).alias(\"inside_ad_flag\").to_frame().write_csv(\n",
    "    \"../output/inside_ad_flag_prediction_ocsvm_gamma_optimization.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd9251f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4-3 アンサンブルSVR\n",
    "\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import KFold, cross_val_predict\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "number_of_sub_datasets = 30  # サブデータセットの数\n",
    "fold_number = 10  # N-fold CV の N\n",
    "\n",
    "svr_cs = 2 ** np.arange(-5, 11, dtype=float)  # C の候補\n",
    "svr_epsilons = 2 ** np.arange(-10, 1, dtype=float)  # ε の候補\n",
    "svr_gammas = 2 ** np.arange(-20, 11, dtype=float)  # γ の候補\n",
    "\n",
    "dataset = pl.read_csv(\"../test_data/resin.csv\")\n",
    "x_prediction = pl.read_csv(\"../test_data/resin_prediction.csv\")\n",
    "\n",
    "# データ分割\n",
    "y = dataset.get_column(\"property\")  # 目的変数\n",
    "x = dataset.drop(y.name)  # 説明変数\n",
    "\n",
    "# 標準偏差が 0 の特徴量の削除\n",
    "deleting_variables = [\n",
    "    col for col, std in x.std().row(0, named=True).items() if std in [0, None]\n",
    "]\n",
    "x = x.drop(deleting_variables)\n",
    "x_prediction = x_prediction.drop(deleting_variables)\n",
    "\n",
    "# オートスケーリング\n",
    "autoscaled_x = x.select((pl.all() - pl.all().mean()) / pl.all().std())\n",
    "autoscaled_x_prediction = (\n",
    "    x_prediction - x.mean().select(pl.all().repeat_by(x_prediction.height).explode())\n",
    ") / x.std().select(pl.all().repeat_by(x_prediction.height).explode())\n",
    "autoscaled_y = (y - y.mean()) / y.std()\n",
    "\n",
    "rate_of_selected_x_variables = (\n",
    "    0.75  # 各サブデータセットで選択される説明変数の数の割合。0 < x < 1\n",
    ")\n",
    "number_of_x_variables = int(np.ceil(x.width * rate_of_selected_x_variables))\n",
    "print(\"各サブデータセットにおける説明変数の数 :\", number_of_x_variables)\n",
    "estimated_y_train_all = []  # サブデータセットごとの y の推定結果を追加\n",
    "selected_x_variable_numbers = []  # 各サブデータセットの説明変数の番号を追加\n",
    "submodels = []  # 構築済みの各サブモデルを追加\n",
    "\n",
    "for submodel_number in range(number_of_sub_datasets):\n",
    "    print(submodel_number + 1, \"/\", number_of_sub_datasets)  # 進捗状況の表示\n",
    "    # 説明変数の選択\n",
    "    # 0 から 1 までの間に一様に分布する乱数を説明変数の数だけ生成して、その乱数値が小さい順に説明変数を選択\n",
    "    random_x_variables = np.random.rand(x.width)\n",
    "    selected_x_variable_numbers_tmp = random_x_variables.argsort()[\n",
    "        :number_of_x_variables\n",
    "    ]\n",
    "    selected_x_columns = [x.columns[i] for i in selected_x_variable_numbers_tmp]\n",
    "    selected_autoscaled_x = autoscaled_x.select(selected_x_columns)\n",
    "    selected_x_variable_numbers.append(selected_x_variable_numbers_tmp)\n",
    "\n",
    "    # ハイパーパラメータの最適化\n",
    "    # 分散最大化によるガウシアンカーネルのγの最適化\n",
    "    variance_of_gram_matrix = []\n",
    "    selected_autoscaled_x_array = selected_autoscaled_x.to_numpy()\n",
    "    for nonlinear_svr_gamma in svr_gammas:\n",
    "        gram_matrix = np.exp(\n",
    "            -nonlinear_svr_gamma\n",
    "            * (\n",
    "                (\n",
    "                    selected_autoscaled_x_array[:, np.newaxis]\n",
    "                    - selected_autoscaled_x_array\n",
    "                )\n",
    "                ** 2\n",
    "            ).sum(axis=2)\n",
    "        )\n",
    "        variance_of_gram_matrix.append(gram_matrix.var(ddof=1))\n",
    "    optimal_svr_gamma = svr_gammas[\n",
    "        np.where(variance_of_gram_matrix == np.max(variance_of_gram_matrix))[0][0]\n",
    "    ]\n",
    "    cross_validation = KFold(\n",
    "        n_splits=fold_number, shuffle=True\n",
    "    )  # クロスバリデーションの分割の設定\n",
    "\n",
    "    # CV による ε の最適化\n",
    "    r2cvs = []  # 空の list。候補ごとに、クロスバリデーション後の r2 を入れていきます\n",
    "    for svr_epsilon in svr_epsilons:\n",
    "        model = SVR(kernel=\"rbf\", C=3, epsilon=svr_epsilon, gamma=optimal_svr_gamma)\n",
    "        autoscaled_estimated_y_in_cv = cross_val_predict(\n",
    "            model,\n",
    "            selected_autoscaled_x.to_numpy(),\n",
    "            autoscaled_y.to_numpy(),\n",
    "            cv=cross_validation,\n",
    "        )\n",
    "        r2cvs.append(\n",
    "            r2_score(y.to_numpy(), autoscaled_estimated_y_in_cv * y.std() + y.mean())\n",
    "        )\n",
    "    optimal_svr_epsilon = svr_epsilons[\n",
    "        np.where(r2cvs == np.max(r2cvs))[0][0]\n",
    "    ]  # クロスバリデーション後の r2 が最も大きい候補\n",
    "\n",
    "    # CV による C の最適化\n",
    "    r2cvs = []  # 空の list。候補ごとに、クロスバリデーション後の r2 を入れていきます\n",
    "    for svr_c in svr_cs:\n",
    "        model = SVR(\n",
    "            kernel=\"rbf\", C=svr_c, epsilon=optimal_svr_epsilon, gamma=optimal_svr_gamma\n",
    "        )\n",
    "        autoscaled_estimated_y_in_cv = cross_val_predict(\n",
    "            model,\n",
    "            selected_autoscaled_x.to_numpy(),\n",
    "            autoscaled_y.to_numpy(),\n",
    "            cv=cross_validation,\n",
    "        )\n",
    "        r2cvs.append(\n",
    "            r2_score(y.to_numpy(), autoscaled_estimated_y_in_cv * y.std() + y.mean())\n",
    "        )\n",
    "    optimal_svr_c = svr_cs[\n",
    "        np.where(r2cvs == np.max(r2cvs))[0][0]\n",
    "    ]  # クロスバリデーション後の r2 が最も大きい候補\n",
    "\n",
    "    # CV による γ の最適化\n",
    "    r2cvs = []  # 空の list。候補ごとに、クロスバリデーション後の r2 を入れていきます\n",
    "    for svr_gamma in svr_gammas:\n",
    "        model = SVR(\n",
    "            kernel=\"rbf\", C=optimal_svr_c, epsilon=optimal_svr_epsilon, gamma=svr_gamma\n",
    "        )\n",
    "        autoscaled_estimated_y_in_cv = cross_val_predict(\n",
    "            model,\n",
    "            selected_autoscaled_x.to_numpy(),\n",
    "            autoscaled_y.to_numpy(),\n",
    "            cv=cross_validation,\n",
    "        )\n",
    "        r2cvs.append(\n",
    "            r2_score(y.to_numpy(), autoscaled_estimated_y_in_cv * y.std() + y.mean())\n",
    "        )\n",
    "    optimal_svr_gamma = svr_gammas[\n",
    "        np.where(r2cvs == np.max(r2cvs))[0][0]\n",
    "    ]  # クロスバリデーション後の r2 が最も大きい候補\n",
    "\n",
    "    # SVR\n",
    "    submodel = SVR(\n",
    "        kernel=\"rbf\",\n",
    "        C=optimal_svr_c,\n",
    "        epsilon=optimal_svr_epsilon,\n",
    "        gamma=optimal_svr_gamma,\n",
    "    )  # モデルの宣言\n",
    "    submodel.fit(\n",
    "        selected_autoscaled_x.to_numpy(), autoscaled_y.to_numpy()\n",
    "    )  # モデルの構築\n",
    "    submodels.append(submodel)\n",
    "\n",
    "# サブデータセットの説明変数の種類やサブデータセットを用いて構築されたモデルを保存\n",
    "with open(\"../output/selected_x_variable_numbers_svr_gaussian.bin\", \"wb\") as f:\n",
    "    pickle.dump(selected_x_variable_numbers, f)\n",
    "with open(\"../output/submodels_svr_gaussian.bin\", \"wb\") as f:\n",
    "    pickle.dump(submodels, f)\n",
    "\n",
    "# サブデータセットの説明変数の種類やサブデータセットを用いて構築されたモデルを読み込み\n",
    "with open(\"../output/selected_x_variable_numbers_svr_gaussian.bin\", \"rb\") as f:\n",
    "    selected_x_variable_numbers = pickle.load(f)\n",
    "with open(\"../output/submodels_svr_gaussian.bin\", \"rb\") as f:\n",
    "    submodels = pickle.load(f)\n",
    "\n",
    "# 予測用データセットの y の推定\n",
    "estimated_y_prediction_all = []  # サブモデルごとの予測用データセットの y の推定結果を追加\n",
    "for submodel_number in range(number_of_sub_datasets):\n",
    "    # 説明変数の選択\n",
    "    selected_x_columns = [\n",
    "        x.columns[i] for i in selected_x_variable_numbers[submodel_number]\n",
    "    ]\n",
    "    selected_autoscaled_x_prediction = autoscaled_x_prediction.select(\n",
    "        selected_x_columns\n",
    "    )\n",
    "    # 予測用データセットの y の推定\n",
    "    estimated_y_prediction = submodels[submodel_number].predict(\n",
    "        selected_autoscaled_x_prediction.to_numpy()\n",
    "    )\n",
    "    estimated_y_prediction = (\n",
    "        estimated_y_prediction * y.std() + y.mean()\n",
    "    )  # スケールをもとに戻します\n",
    "    estimated_y_prediction_all.append(estimated_y_prediction)\n",
    "\n",
    "# 予測用データセットの推定値の平均値\n",
    "estimated_y_prediction_all_df = pl.DataFrame(estimated_y_prediction_all).transpose()\n",
    "estimated_y_prediction = estimated_y_prediction_all_df.mean_horizontal().alias(\n",
    "    \"estimated_y\"\n",
    ")\n",
    "estimated_y_prediction.to_frame().write_csv(\n",
    "    \"../output/estimated_y_prediction_ensemble_svr_gaussian.csv\"\n",
    ")\n",
    "\n",
    "# 予測用データセットの推定値の標準偏差\n",
    "std_of_estimated_y_prediction = estimated_y_prediction_all_df.std(1).alias(\n",
    "    \"std_of_estimated_y\"\n",
    ")\n",
    "std_of_estimated_y_prediction.to_frame().write_csv(\n",
    "    \"../output/std_of_estimated_y_prediction_ensemble_svr_gaussian.csv\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python-doe-kspub",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
